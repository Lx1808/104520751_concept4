{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codexnyctis/104520751_concept4/blob/dev%2Fnur%2FML_training/classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "akg--CKuMtSp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": true,
        "id": "qvxNhVH6FBmy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import json\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "N0252fNFMy8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('Obfuscated-MalMem2022.csv')"
      ],
      "metadata": {
        "id": "ghMphUR0dYOs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract malware type and family from the 'category' column\n",
        "def extract_malware_info(category):\n",
        "    if category == 'Benign':\n",
        "        return 'Benign', 'Benign'\n",
        "    parts = category.split('-')\n",
        "    if len(parts) >= 2:\n",
        "        return parts[0], parts[1]\n",
        "    return 'Unknown', 'Unknown'\n",
        "\n",
        "df['Malware_Type'], df['Malware_Family'] = zip(*df['Category'].apply(extract_malware_info))"
      ],
      "metadata": {
        "id": "2QsNl5QYd3Lx"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Presentation Set"
      ],
      "metadata": {
        "id": "lmE5GqQSd_FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract presentation set using stratified sampling\n",
        "df_main, df_present = train_test_split(\n",
        "    df, test_size=0.002, stratify=df['Class'], random_state=42\n",
        ")\n",
        "\n",
        "# Save the presentation set to a new CSV file\n",
        "presentation_file = 'presentation_samples.csv'\n",
        "df_present.to_csv(presentation_file, index=False)\n",
        "\n",
        "print(f\"Presentation set saved to {presentation_file}\")\n",
        "print(f\"Number of samples in presentation set: {len(df_present)}\")\n",
        "\n",
        "# Display class distribution in the presentation set\n",
        "print(\"\\nClass distribution in presentation set:\")\n",
        "print(df_present['Class'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nClass distribution in main dataset:\")\n",
        "print(df_main['Class'].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG_Sojknd-Pn",
        "outputId": "89da6957-a1bf-4626-a2c2-5db46c4d6420"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Presentation set saved to presentation_samples.csv\n",
            "Number of samples in presentation set: 118\n",
            "\n",
            "Class distribution in presentation set:\n",
            "Class\n",
            "Benign     0.5\n",
            "Malware    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Class distribution in main dataset:\n",
            "Class\n",
            "Benign     0.5\n",
            "Malware    0.5\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aVWlr8QMNro",
        "outputId": "e25ce012-6799-42f2-db42-937dc27bfe5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape after removing constant features: (58478, 52)\n",
            "\n",
            "Shape after feature selection: (58478, 16)\n",
            "Selected features: ['pslist.avg_threads', 'dlllist.ndlls', 'dlllist.avg_dlls_per_proc', 'handles.nevent', 'handles.nkey', 'handles.nthread', 'handles.nsemaphore', 'handles.ntimer', 'handles.nsection', 'handles.nmutant', 'ldrmodules.not_in_load', 'ldrmodules.not_in_init', 'ldrmodules.not_in_mem', 'svcscan.process_services', 'svcscan.shared_process_services', 'svcscan.nactive']\n",
            "\n",
            "Final number of features after preprocessing: 16\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the data\n",
        "def preprocess_data(df):\n",
        "    # Separate features and targets from the file: We need to distinguish between input features and output labels\n",
        "    X = df.drop(['Category', 'Class', 'Malware_Type', 'Malware_Family'], axis=1)\n",
        "    y_binary = df['Class']\n",
        "    y_4class = df['Malware_Type']\n",
        "    y_16class = df['Malware_Family']\n",
        "\n",
        "    # Label encode the targets: Converts categorical labels into numerical format\n",
        "    le_binary = LabelEncoder()\n",
        "    le_4class = LabelEncoder()\n",
        "    le_16class = LabelEncoder()\n",
        "    y_binary = le_binary.fit_transform(y_binary)\n",
        "    y_4class = le_4class.fit_transform(y_4class)\n",
        "    y_16class = le_16class.fit_transform(y_16class)\n",
        "\n",
        "    # Remove constant features: Features that don't vary across samples don't provide useful information\n",
        "    variance_selector = VarianceThreshold()\n",
        "    X_var = variance_selector.fit_transform(X)\n",
        "    X_var = pd.DataFrame(X_var, columns=X.columns[variance_selector.get_support()])\n",
        "\n",
        "    print(\"\\nShape after removing constant features:\", X_var.shape)\n",
        "\n",
        "    # Select top 16 features: Helps in dimensionality reduction\n",
        "    feature_selector = SelectKBest(score_func=f_classif, k=min(16, X_var.shape[1]))\n",
        "    try:\n",
        "        X_selected = feature_selector.fit_transform(X_var, y_binary)\n",
        "        selected_features = X_var.columns[feature_selector.get_support()].tolist()\n",
        "    except:\n",
        "        print(\"Error in feature selection. Using all non-constant features.\")\n",
        "        X_selected = X_var\n",
        "        selected_features = X_var.columns.tolist()\n",
        "\n",
        "    X_new = pd.DataFrame(X_selected, columns=selected_features)\n",
        "\n",
        "    print(\"\\nShape after feature selection:\", X_new.shape)\n",
        "    print(\"Selected features:\", selected_features)\n",
        "\n",
        "    # Standardise the features: Ensures all features are on the same scale\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_new)\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=X_new.columns)\n",
        "\n",
        "    print(f\"\\nFinal number of features after preprocessing: {X_scaled.shape[1]}\")\n",
        "\n",
        "    return X_scaled, y_binary, y_4class, y_16class, le_binary, le_4class, le_16class, scaler, feature_selector, variance_selector\n",
        "\n",
        "# This is to take the variables to the global scope\n",
        "X_scaled, y_binary, y_4class, y_16class, le_binary, le_4class, le_16class, scaler, feature_selector, variance_selector = preprocess_data(df_main)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Analysis**"
      ],
      "metadata": {
        "id": "iLKAG9JWNELC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To see if the preprocessing went correct and to observe class imbalance\n",
        "print(\"\\nData Analysis on Main Dataset (After Extracting Presentation Set):\")\n",
        "print(\"\\nDistribution of binary classes:\")\n",
        "print(df_main['Class'].value_counts())\n",
        "\n",
        "print(\"\\nDistribution of 4-class categories:\")\n",
        "print(df_main['Malware_Type'].value_counts())\n",
        "\n",
        "print(\"\\nDistribution of 16-class categories:\")\n",
        "print(df_main['Malware_Family'].value_counts())\n",
        "\n",
        "benign_percentage = (df_main['Class'] == 'Benign').mean() * 100\n",
        "print(f\"\\nPercentage of benign samples: {benign_percentage:.2f}%\")\n",
        "print(f\"Percentage of malicious samples: {100 - benign_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2oUmFek-oiB",
        "outputId": "0b37bbc4-9602-45a8-b5d9-05d5c77d4d2f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data Analysis on Main Dataset (After Extracting Presentation Set):\n",
            "\n",
            "Distribution of binary classes:\n",
            "Class\n",
            "Benign     29239\n",
            "Malware    29239\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of 4-class categories:\n",
            "Malware_Type\n",
            "Benign        29239\n",
            "Spyware        9998\n",
            "Ransomware     9770\n",
            "Trojan         9471\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of 16-class categories:\n",
            "Malware_Family\n",
            "Benign          29239\n",
            "Transponder      2402\n",
            "Gator            2194\n",
            "Shade            2125\n",
            "Refroso          2000\n",
            "Ako              1998\n",
            "180solutions     1997\n",
            "CWS              1997\n",
            "Scar             1993\n",
            "Conti            1979\n",
            "Emotet           1963\n",
            "Maze             1954\n",
            "Zeus             1946\n",
            "Pysa             1714\n",
            "Reconyc          1569\n",
            "TIBS             1408\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentage of benign samples: 50.00%\n",
            "Percentage of malicious samples: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation"
      ],
      "metadata": {
        "id": "v1K-lD8TNNO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "HIrJnwnkMRuq"
      },
      "outputs": [],
      "source": [
        "# Create base models: Three different types of ensemble classifiers\n",
        "def create_base_models():\n",
        "    return [\n",
        "        # Random Forest with 100 trees\n",
        "        RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        # XGBoost with 100 boosting rounds\n",
        "        XGBClassifier(n_estimators=100, random_state=42),\n",
        "        # Extra Trees with 100 trees\n",
        "        ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "    ]\n",
        "\n",
        "# Create meta-learner: A neural network to combine predictions from base models\n",
        "def create_meta_learner(n_classes, n_features):\n",
        "    model = Sequential([\n",
        "        Input(shape=(n_features,)),\n",
        "        # First hidden layer with 256 neurons and ReLU activation\n",
        "        Dense(256, activation='relu'),\n",
        "        # Second hidden layer with 128 neurons and ReLU activation\n",
        "        Dense(128, activation='relu')\n",
        "    ])\n",
        "    # Output layer based on the number of classes\n",
        "    if n_classes == 2:\n",
        "        # Binary classification: single neuron with sigmoid activation\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    else:\n",
        "        # Multi-class classification: softmax activation for multiple classes\n",
        "        model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "MjrwyMOfNXql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dngGkvnrMVUZ"
      },
      "outputs": [],
      "source": [
        "# Run a single experiment with cross-validation\n",
        "def run_experiment(X, y, n_classes, seed):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Initialize stratified k-fold cross-validation\n",
        "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
        "    results = []\n",
        "    best_accuracy = 0\n",
        "    best_models = None\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"Fold {fold}\")\n",
        "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "        # Train and predict with base models\n",
        "        base_models = create_base_models()\n",
        "        base_predictions = []\n",
        "        for model in base_models:\n",
        "            model.fit(X_train, y_train)\n",
        "            if n_classes == 2:\n",
        "                base_predictions.append(model.predict_proba(X_val)[:, 1].reshape(-1, 1))\n",
        "            else:\n",
        "                base_predictions.append(model.predict_proba(X_val))\n",
        "\n",
        "        meta_features = np.hstack(base_predictions)\n",
        "\n",
        "        # Train meta-learner on base model predictions\n",
        "        meta_learner = create_meta_learner(n_classes, meta_features.shape[1])\n",
        "        meta_learner.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy' if n_classes == 2 else 'sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        meta_learner.fit(meta_features, y_val, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "        # Make final predictions using the trained meta-learner\n",
        "        final_predictions = meta_learner.predict(meta_features)\n",
        "        if n_classes == 2:\n",
        "            final_predictions = (final_predictions > 0.5).astype(int).flatten()\n",
        "        else:\n",
        "            final_predictions = np.argmax(final_predictions, axis=1)\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        accuracy = accuracy_score(y_val, final_predictions)\n",
        "        precision = precision_score(y_val, final_predictions, average='binary' if n_classes == 2 else 'weighted')\n",
        "        recall = recall_score(y_val, final_predictions, average='binary' if n_classes == 2 else 'weighted')\n",
        "        f1 = f1_score(y_val, final_predictions, average='binary' if n_classes == 2 else 'weighted')\n",
        "\n",
        "        results.append((accuracy, precision, recall, f1))\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_models = (base_models, meta_learner)\n",
        "\n",
        "    return np.mean(results, axis=0), best_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "3VK2fZvLMaqn"
      },
      "outputs": [],
      "source": [
        "# Run multiple experiments and average the results\n",
        "def run_multiple_experiments(X, y, n_classes, n_runs=5):\n",
        "    all_results = []\n",
        "    best_overall_accuracy = 0\n",
        "    best_overall_models = None\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}\")\n",
        "        results, models = run_experiment(X, y, n_classes, seed=run)\n",
        "        all_results.append(results)\n",
        "        if results[0] > best_overall_accuracy:\n",
        "            best_overall_accuracy = results[0]\n",
        "            best_overall_models = models\n",
        "    return np.mean(all_results, axis=0), best_overall_models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation"
      ],
      "metadata": {
        "id": "vUoNDoemNgMU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fbUQLsRqMd9F",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Print final results\n",
        "def print_results(results, classification_type):\n",
        "    print(f\"\\n{classification_type} Classification Results:\")\n",
        "    print(f\"Accuracy: {results[0]:.4f}\")\n",
        "    print(f\"Precision: {results[1]:.4f}\")\n",
        "    print(f\"Recall: {results[2]:.4f}\")\n",
        "    print(f\"F1-score: {results[3]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Binary Classification\")\n",
        "binary_results, binary_models = run_multiple_experiments(X_scaled, pd.Series(y_binary), 2)\n",
        "print_results(binary_results, \"Binary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95N1Ue-QDjC_",
        "outputId": "38512cbc-3811-4095-9103-caab42258c29"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Classification\n",
            "\n",
            "Run 1\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 2\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 3\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 4\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 5\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Binary Classification Results:\n",
            "Accuracy: 1.0000\n",
            "Precision: 0.9999\n",
            "Recall: 1.0000\n",
            "F1-score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n4-Class Classification\")\n",
        "results_4class, models_4class = run_multiple_experiments(X_scaled, pd.Series(y_4class), 4)\n",
        "print_results(results_4class, \"4-Class\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex7Wev4Ovz5m",
        "outputId": "ea167dc1-fba8-4e0c-832e-2a14911f3615"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4-Class Classification\n",
            "\n",
            "Run 1\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 2\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 3\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 4\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 5\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "4-Class Classification Results:\n",
            "Accuracy: 0.8926\n",
            "Precision: 0.8927\n",
            "Recall: 0.8926\n",
            "F1-score: 0.8922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n16-Class Classification\")\n",
        "results_16class, models_16class = run_multiple_experiments(X_scaled, pd.Series(y_16class), 16)\n",
        "print_results(results_16class, \"16-Class\")"
      ],
      "metadata": {
        "id": "Wh5GviGqwBTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ea8b78-57f4-4cb9-f182-76f43956b6cc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "16-Class Classification\n",
            "\n",
            "Run 1\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 2\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Run 3\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Run 4\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 5\n",
            "Fold 1\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m457/457\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "16-Class Classification Results:\n",
            "Accuracy: 0.9169\n",
            "Precision: 0.9221\n",
            "Recall: 0.9169\n",
            "F1-score: 0.9165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Saving"
      ],
      "metadata": {
        "id": "GP_7DUR-Npvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Saving\n",
        "def save_model_components(base_models, meta_learner, le_binary, le_4class, le_16class, scaler, feature_selector, variance_selector, selected_features, save_path):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Save base models\n",
        "    for i, model in enumerate(base_models):\n",
        "        joblib.dump(model, f\"{save_path}/base_model_{i}.joblib\")\n",
        "\n",
        "    # Save meta-learner with .keras extension\n",
        "    tf.keras.models.save_model(meta_learner, f\"{save_path}/meta_learner.keras\")\n",
        "\n",
        "    # Save preprocessors\n",
        "    joblib.dump(le_binary, f\"{save_path}/le_binary.joblib\")\n",
        "    joblib.dump(le_4class, f\"{save_path}/le_4class.joblib\")\n",
        "    joblib.dump(le_16class, f\"{save_path}/le_16class.joblib\")\n",
        "    joblib.dump(scaler, f\"{save_path}/scaler.joblib\")\n",
        "    joblib.dump(feature_selector, f\"{save_path}/feature_selector.joblib\")\n",
        "    joblib.dump(variance_selector, f\"{save_path}/variance_selector.joblib\")\n",
        "\n",
        "    # Save selected features\n",
        "    with open(f\"{save_path}/selected_features.json\", 'w') as f:\n",
        "        json.dump(selected_features, f)\n",
        "\n",
        "    print(f\"Model components saved to {save_path}\")"
      ],
      "metadata": {
        "id": "k5yxKWsaDCy_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save models\n",
        "save_model_components(\n",
        "    binary_models[0],  # base models\n",
        "    binary_models[1],  # meta learner\n",
        "    le_binary,\n",
        "    le_4class,\n",
        "    le_16class,\n",
        "    scaler,\n",
        "    feature_selector,\n",
        "    variance_selector,\n",
        "    X_scaled.columns.tolist(),  # selected features\n",
        "    save_path=\"saved_model_binary\"\n",
        ")\n",
        "\n",
        "save_model_components(\n",
        "    models_4class[0],\n",
        "    models_4class[1],\n",
        "    le_binary,\n",
        "    le_4class,\n",
        "    le_16class,\n",
        "    scaler,\n",
        "    feature_selector,\n",
        "    variance_selector,\n",
        "    X_scaled.columns.tolist(),\n",
        "    save_path=\"saved_model_4class\"\n",
        ")\n",
        "\n",
        "save_model_components(\n",
        "    models_16class[0],\n",
        "    models_16class[1],\n",
        "    le_binary,\n",
        "    le_4class,\n",
        "    le_16class,\n",
        "    scaler,\n",
        "    feature_selector,\n",
        "    variance_selector,\n",
        "    X_scaled.columns.tolist(),\n",
        "    save_path=\"saved_model_16class\"\n",
        ")\n",
        "\n",
        "print(\"All models have been saved.\")\n",
        "\n",
        "# Download the saved models from Google Colab:\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r saved_models.zip saved_model_binary saved_model_4class saved_model_16class\n",
        "files.download('saved_models.zip')"
      ],
      "metadata": {
        "id": "i1K5Br6jDpe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "outputId": "e7b58655-b7d5-42bb-a934-7e90528470de",
        "collapsed": true
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model components saved to saved_model_binary\n",
            "Model components saved to saved_model_4class\n",
            "Model components saved to saved_model_16class\n",
            "All models have been saved.\n",
            "  adding: saved_model_binary/ (stored 0%)\n",
            "  adding: saved_model_binary/base_model_1.joblib (deflated 82%)\n",
            "  adding: saved_model_binary/le_16class.joblib (deflated 30%)\n",
            "  adding: saved_model_binary/feature_selector.joblib (deflated 44%)\n",
            "  adding: saved_model_binary/base_model_0.joblib (deflated 76%)\n",
            "  adding: saved_model_binary/base_model_2.joblib (deflated 74%)\n",
            "  adding: saved_model_binary/selected_features.json (deflated 59%)\n",
            "  adding: saved_model_binary/le_binary.joblib (deflated 28%)\n",
            "  adding: saved_model_binary/variance_selector.joblib (deflated 46%)\n",
            "  adding: saved_model_binary/scaler.joblib (deflated 31%)\n",
            "  adding: saved_model_binary/le_4class.joblib (deflated 28%)\n",
            "  adding: saved_model_binary/meta_learner.keras (deflated 25%)\n",
            "  adding: saved_model_4class/ (stored 0%)\n",
            "  adding: saved_model_4class/base_model_1.joblib (deflated 67%)\n",
            "  adding: saved_model_4class/le_16class.joblib (deflated 30%)\n",
            "  adding: saved_model_4class/feature_selector.joblib (deflated 44%)\n",
            "  adding: saved_model_4class/base_model_0.joblib (deflated 85%)\n",
            "  adding: saved_model_4class/base_model_2.joblib (deflated 85%)\n",
            "  adding: saved_model_4class/selected_features.json (deflated 59%)\n",
            "  adding: saved_model_4class/le_binary.joblib (deflated 28%)\n",
            "  adding: saved_model_4class/variance_selector.joblib (deflated 46%)\n",
            "  adding: saved_model_4class/scaler.joblib (deflated 31%)\n",
            "  adding: saved_model_4class/le_4class.joblib (deflated 28%)\n",
            "  adding: saved_model_4class/meta_learner.keras (deflated 11%)\n",
            "  adding: saved_model_16class/ (stored 0%)\n",
            "  adding: saved_model_16class/base_model_1.joblib (deflated 67%)\n",
            "  adding: saved_model_16class/le_16class.joblib (deflated 30%)\n",
            "  adding: saved_model_16class/feature_selector.joblib (deflated 44%)\n",
            "  adding: saved_model_16class/base_model_0.joblib (deflated 90%)\n",
            "  adding: saved_model_16class/base_model_2.joblib (deflated 91%)\n",
            "  adding: saved_model_16class/selected_features.json (deflated 59%)\n",
            "  adding: saved_model_16class/le_binary.joblib (deflated 28%)\n",
            "  adding: saved_model_16class/variance_selector.joblib (deflated 46%)\n",
            "  adding: saved_model_16class/scaler.joblib (deflated 31%)\n",
            "  adding: saved_model_16class/le_4class.joblib (deflated 28%)\n",
            "  adding: saved_model_16class/meta_learner.keras (deflated 10%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5d304b01-3dbd-4243-89d1-795f86c8c906\", \"saved_models.zip\", 134759404)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "akg--CKuMtSp",
        "N0252fNFMy8V",
        "MjrwyMOfNXql"
      ],
      "authorship_tag": "ABX9TyNaJDrt75i5FRXfqhkCPurJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}