{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codexnyctis/104520751_concept4/blob/dev%2Fnur%2FML_training/stackingEnsemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "akg--CKuMtSp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "qvxNhVH6FBmy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "import json\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "N0252fNFMy8V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aVWlr8QMNro",
        "outputId": "6af41b83-9051-43a5-e33b-e0065948eb50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape after removing constant features: (58596, 52)\n",
            "\n",
            "Shape after feature selection: (58596, 16)\n",
            "Selected features: ['pslist.avg_threads', 'dlllist.ndlls', 'dlllist.avg_dlls_per_proc', 'handles.nevent', 'handles.nkey', 'handles.nthread', 'handles.nsemaphore', 'handles.ntimer', 'handles.nsection', 'handles.nmutant', 'ldrmodules.not_in_load', 'ldrmodules.not_in_init', 'ldrmodules.not_in_mem', 'svcscan.process_services', 'svcscan.shared_process_services', 'svcscan.nactive']\n",
            "\n",
            "Final number of features after preprocessing: 16\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('Obfuscated-MalMem2022.csv')\n",
        "\n",
        "# Extract malware type and family from the 'category' column\n",
        "def extract_malware_info(category):\n",
        "    if category == 'Benign':\n",
        "        return 'Benign', 'Benign'\n",
        "    parts = category.split('-')\n",
        "    if len(parts) >= 2:\n",
        "        return parts[0], parts[1]\n",
        "    return 'Unknown', 'Unknown'\n",
        "\n",
        "df['Malware_Type'], df['Malware_Family'] = zip(*df['Category'].apply(extract_malware_info))\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(df):\n",
        "    # Separate features and targets from the file: We need to distinguish between input features and output labels\n",
        "    X = df.drop(['Category', 'Class', 'Malware_Type', 'Malware_Family'], axis=1)\n",
        "    y_binary = df['Class']\n",
        "    y_4class = df['Malware_Type']\n",
        "    y_16class = df['Malware_Family']\n",
        "\n",
        "    # Label encode the targets: Converts categorical labels into numerical format\n",
        "    le_binary = LabelEncoder()\n",
        "    le_4class = LabelEncoder()\n",
        "    le_16class = LabelEncoder()\n",
        "    y_binary = le_binary.fit_transform(y_binary)\n",
        "    y_4class = le_4class.fit_transform(y_4class)\n",
        "    y_16class = le_16class.fit_transform(y_16class)\n",
        "\n",
        "    # Remove constant features: Features that don't vary across samples don't provide useful information\n",
        "    variance_selector = VarianceThreshold()\n",
        "    X_var = variance_selector.fit_transform(X)\n",
        "    X_var = pd.DataFrame(X_var, columns=X.columns[variance_selector.get_support()])\n",
        "\n",
        "    print(\"\\nShape after removing constant features:\", X_var.shape)\n",
        "\n",
        "    # Select top 16 features: Helps in dimensionality reduction\n",
        "    feature_selector = SelectKBest(score_func=f_classif, k=min(16, X_var.shape[1]))\n",
        "    try:\n",
        "        X_selected = feature_selector.fit_transform(X_var, y_binary)\n",
        "        selected_features = X_var.columns[feature_selector.get_support()].tolist()\n",
        "    except:\n",
        "        print(\"Error in feature selection. Using all non-constant features.\")\n",
        "        X_selected = X_var\n",
        "        selected_features = X_var.columns.tolist()\n",
        "\n",
        "    X_new = pd.DataFrame(X_selected, columns=selected_features)\n",
        "\n",
        "    print(\"\\nShape after feature selection:\", X_new.shape)\n",
        "    print(\"Selected features:\", selected_features)\n",
        "\n",
        "    # Standardise the features: Ensures all features are on the same scale\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_new)\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=X_new.columns)\n",
        "\n",
        "    print(f\"\\nFinal number of features after preprocessing: {X_scaled.shape[1]}\")\n",
        "\n",
        "    return X_scaled, y_binary, y_4class, y_16class, le_binary, le_4class, le_16class, scaler, feature_selector, variance_selector\n",
        "\n",
        "# This is to take the variables to the global scope\n",
        "X_scaled, y_binary, y_4class, y_16class, le_binary, le_4class, le_16class, scaler, feature_selector, variance_selector = preprocess_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Analysis**"
      ],
      "metadata": {
        "id": "iLKAG9JWNELC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To see if the preprocessing went correct and to observe class imbalance\n",
        "print(\"\\nDistribution of binary classes:\")\n",
        "print(df['Class'].value_counts())\n",
        "\n",
        "print(\"\\nDistribution of 4-class categories:\")\n",
        "print(df['Malware_Type'].value_counts())\n",
        "\n",
        "print(\"\\nDistribution of 16-class categories:\")\n",
        "print(df['Malware_Family'].value_counts())\n",
        "\n",
        "benign_percentage = (df['Class'] == 'Benign').mean() * 100\n",
        "print(f\"\\nPercentage of benign samples: {benign_percentage:.2f}%\")\n",
        "print(f\"Percentage of malicious samples: {100 - benign_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2oUmFek-oiB",
        "outputId": "d2e8c084-ae36-431c-b027-2c1cd0ee5131"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Distribution of binary classes:\n",
            "Class\n",
            "Benign     29298\n",
            "Malware    29298\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of 4-class categories:\n",
            "Malware_Type\n",
            "Benign        29298\n",
            "Spyware       10020\n",
            "Ransomware     9791\n",
            "Trojan         9487\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribution of 16-class categories:\n",
            "Malware_Family\n",
            "Benign          29298\n",
            "Transponder      2410\n",
            "Gator            2200\n",
            "Shade            2128\n",
            "Ako              2000\n",
            "180solutions     2000\n",
            "CWS              2000\n",
            "Refroso          2000\n",
            "Scar             2000\n",
            "Conti            1988\n",
            "Emotet           1967\n",
            "Maze             1958\n",
            "Zeus             1950\n",
            "Pysa             1717\n",
            "Reconyc          1570\n",
            "TIBS             1410\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentage of benign samples: 50.00%\n",
            "Percentage of malicious samples: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation"
      ],
      "metadata": {
        "id": "v1K-lD8TNNO3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIrJnwnkMRuq"
      },
      "outputs": [],
      "source": [
        "# Create base models: Three different types of ensemble classifiers\n",
        "def create_base_models():\n",
        "    return [\n",
        "        # Random Forest with 100 trees\n",
        "        RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        # XGBoost with 100 boosting rounds\n",
        "        XGBClassifier(n_estimators=100, random_state=42),\n",
        "        # Extra Trees with 100 trees\n",
        "        ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "    ]\n",
        "\n",
        "# Create meta-learner: A neural network to combine predictions from base models\n",
        "def create_meta_learner(n_classes, n_features):\n",
        "    model = Sequential([\n",
        "        Input(shape=(n_features,)),\n",
        "        # First hidden layer with 256 neurons and ReLU activation\n",
        "        Dense(256, activation='relu'),\n",
        "        # Second hidden layer with 128 neurons and ReLU activation\n",
        "        Dense(128, activation='relu')\n",
        "    ])\n",
        "    # Output layer based on the number of classes\n",
        "    if n_classes == 2:\n",
        "        # Binary classification: single neuron with sigmoid activation\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "    else:\n",
        "        # Multi-class classification: softmax activation for multiple classes\n",
        "        model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "MjrwyMOfNXql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dngGkvnrMVUZ"
      },
      "outputs": [],
      "source": [
        "# Run a single experiment with cross-validation\n",
        "def run_experiment(X, y, n_classes, seed):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Initialize stratified k-fold cross-validation\n",
        "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=seed)\n",
        "    results = []\n",
        "    best_accuracy = 0\n",
        "    best_models = None\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"Fold {fold}\")\n",
        "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "\n",
        "        # Train and predict with base models\n",
        "        base_models = create_base_models()\n",
        "        base_predictions = []\n",
        "        for model in base_models:\n",
        "            model.fit(X_train, y_train)\n",
        "            if n_classes == 2:\n",
        "                base_predictions.append(model.predict_proba(X_val)[:, 1].reshape(-1, 1))\n",
        "            else:\n",
        "                base_predictions.append(model.predict_proba(X_val))\n",
        "\n",
        "        meta_features = np.hstack(base_predictions)\n",
        "\n",
        "        # Train meta-learner on base model predictions\n",
        "        meta_learner = create_meta_learner(n_classes, meta_features.shape[1])\n",
        "        meta_learner.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy' if n_classes == 2 else 'sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        meta_learner.fit(meta_features, y_val, epochs=100, batch_size=32, verbose=0)\n",
        "\n",
        "        # Make final predictions using the trained meta-learner\n",
        "        final_predictions = meta_learner.predict(meta_features)\n",
        "        if n_classes == 2:\n",
        "            final_predictions = (final_predictions > 0.5).astype(int).flatten()\n",
        "        else:\n",
        "            final_predictions = np.argmax(final_predictions, axis=1)\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        accuracy = accuracy_score(y_val, final_predictions)\n",
        "        precision = precision_score(y_val, final_predictions, average='binary' if n_classes == 2 else 'weighted')\n",
        "        recall = recall_score(y_val, final_predictions, average='binary' if n_classes == 2 else 'weighted')\n",
        "        f1 = f1_score(y_val, final_predictions, average='binary' if n_classes == 2 else 'weighted')\n",
        "\n",
        "        results.append((accuracy, precision, recall, f1))\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_models = (base_models, meta_learner)\n",
        "\n",
        "    return np.mean(results, axis=0), best_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VK2fZvLMaqn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Run multiple experiments and average the results\n",
        "def run_multiple_experiments(X, y, n_classes, n_runs=5):\n",
        "    all_results = []\n",
        "    best_overall_accuracy = 0\n",
        "    best_overall_models = None\n",
        "    for run in range(n_runs):\n",
        "        print(f\"\\nRun {run + 1}\")\n",
        "        results, models = run_experiment(X, y, n_classes, seed=run)\n",
        "        all_results.append(results)\n",
        "        if results[0] > best_overall_accuracy:\n",
        "            best_overall_accuracy = results[0]\n",
        "            best_overall_models = models\n",
        "    return np.mean(all_results, axis=0), best_overall_models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation"
      ],
      "metadata": {
        "id": "vUoNDoemNgMU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbUQLsRqMd9F",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Print final results\n",
        "def print_results(results, classification_type):\n",
        "    print(f\"\\n{classification_type} Classification Results:\")\n",
        "    print(f\"Accuracy: {results[0]:.4f}\")\n",
        "    print(f\"Precision: {results[1]:.4f}\")\n",
        "    print(f\"Recall: {results[2]:.4f}\")\n",
        "    print(f\"F1-score: {results[3]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Binary Classification\")\n",
        "binary_results, binary_models = run_multiple_experiments(X_scaled, pd.Series(y_binary), 2)\n",
        "print_results(binary_results, \"Binary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95N1Ue-QDjC_",
        "outputId": "864147e3-8535-462d-b90a-eb44d4bbca57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Classification\n",
            "\n",
            "Run 1\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 2\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 3\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 4\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Run 5\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Binary Classification Results:\n",
            "Accuracy: 1.0000\n",
            "Precision: 1.0000\n",
            "Recall: 0.9999\n",
            "F1-score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n4-Class Classification\")\n",
        "results_4class, models_4class = run_multiple_experiments(X_scaled, pd.Series(y_4class), 4)\n",
        "print_results(results_4class, \"4-Class\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex7Wev4Ovz5m",
        "outputId": "a820e6c8-3180-4bd4-d1c1-e2441c922816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4-Class Classification\n",
            "\n",
            "Run 1\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Run 2\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 3\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Run 4\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Run 5\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "4-Class Classification Results:\n",
            "Accuracy: 0.8923\n",
            "Precision: 0.8927\n",
            "Recall: 0.8923\n",
            "F1-score: 0.8919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n16-Class Classification\")\n",
        "results_16class, models_16class = run_multiple_experiments(X_scaled, pd.Series(y_16class), 16)\n",
        "print_results(results_16class, \"16-Class\")"
      ],
      "metadata": {
        "id": "Wh5GviGqwBTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea1e809-4f51-4420-badd-9754079ca20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "16-Class Classification\n",
            "\n",
            "Run 1\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 2\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 3\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 4\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "Run 5\n",
            "Fold 1\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 2\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 3\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Fold 4\n",
            "\u001b[1m458/458\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "16-Class Classification Results:\n",
            "Accuracy: 0.9188\n",
            "Precision: 0.9234\n",
            "Recall: 0.9188\n",
            "F1-score: 0.9183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Saving"
      ],
      "metadata": {
        "id": "GP_7DUR-Npvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from tensorflow.keras.models import save_model\n",
        "import json\n",
        "import os\n",
        "\n",
        "def save_model_components(base_models, meta_learner, le_binary, le_4class, le_16class, scaler, feature_selector, variance_selector, selected_features, save_path):\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Save base models\n",
        "    for i, model in enumerate(base_models):\n",
        "        joblib.dump(model, f\"{save_path}/base_model_{i}.joblib\")\n",
        "\n",
        "    # Save meta-learner\n",
        "    save_model(meta_learner, f\"{save_path}/meta_learner.h5\")\n",
        "\n",
        "    # Save preprocessors\n",
        "    joblib.dump(le_binary, f\"{save_path}/le_binary.joblib\")\n",
        "    joblib.dump(le_4class, f\"{save_path}/le_4class.joblib\")\n",
        "    joblib.dump(le_16class, f\"{save_path}/le_16class.joblib\")\n",
        "    joblib.dump(scaler, f\"{save_path}/scaler.joblib\")\n",
        "    joblib.dump(feature_selector, f\"{save_path}/feature_selector.joblib\")\n",
        "    joblib.dump(variance_selector, f\"{save_path}/variance_selector.joblib\")\n",
        "\n",
        "    # Save selected features\n",
        "    with open(f\"{save_path}/selected_features.json\", 'w') as f:\n",
        "        json.dump(selected_features, f)\n",
        "\n",
        "    print(f\"Model components saved to {save_path}\")"
      ],
      "metadata": {
        "id": "k5yxKWsaDCy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best models\n",
        "save_model_components(\n",
        "    binary_models[0],  # base models\n",
        "    binary_models[1],  # meta learner\n",
        "    le_binary,\n",
        "    le_4class,\n",
        "    le_16class,\n",
        "    scaler,\n",
        "    feature_selector,\n",
        "    variance_selector,\n",
        "    X_scaled.columns.tolist(),  # selected features\n",
        "    save_path=\"/content/saved_model_binary\"\n",
        ")\n",
        "\n",
        "save_model_components(\n",
        "    models_4class[0],\n",
        "    models_4class[1],\n",
        "    le_binary,\n",
        "    le_4class,\n",
        "    le_16class,\n",
        "    scaler,\n",
        "    feature_selector,\n",
        "    variance_selector,\n",
        "    X_scaled.columns.tolist(),\n",
        "    save_path=\"/content/saved_model_4class\"\n",
        ")\n",
        "\n",
        "save_model_components(\n",
        "    models_16class[0],\n",
        "    models_16class[1],\n",
        "    le_binary,\n",
        "    le_4class,\n",
        "    le_16class,\n",
        "    scaler,\n",
        "    feature_selector,\n",
        "    variance_selector,\n",
        "    X_scaled.columns.tolist(),\n",
        "    save_path=\"/content/saved_model_16class\"\n",
        ")\n",
        "\n",
        "print(\"All models have been saved.\")\n",
        "\n",
        "# To download the saved models from Google Colab, use:\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r saved_models.zip /content/saved_model_binary /content/saved_model_4class /content/saved_model_16class\n",
        "files.download('saved_models.zip')"
      ],
      "metadata": {
        "id": "i1K5Br6jDpe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "fa5bf890-588c-4c82-b985-9e43b9f7e3b9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model components saved to /content/saved_model_binary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model components saved to /content/saved_model_4class\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model components saved to /content/saved_model_16class\n",
            "All models have been saved.\n",
            "  adding: content/saved_model_binary/ (stored 0%)\n",
            "  adding: content/saved_model_binary/meta_learner.h5 (deflated 24%)\n",
            "  adding: content/saved_model_binary/selected_features.json (deflated 59%)\n",
            "  adding: content/saved_model_binary/feature_selector.joblib (deflated 44%)\n",
            "  adding: content/saved_model_binary/variance_selector.joblib (deflated 46%)\n",
            "  adding: content/saved_model_binary/base_model_2.joblib (deflated 78%)\n",
            "  adding: content/saved_model_binary/base_model_0.joblib (deflated 79%)\n",
            "  adding: content/saved_model_binary/le_binary.joblib (deflated 28%)\n",
            "  adding: content/saved_model_binary/scaler.joblib (deflated 31%)\n",
            "  adding: content/saved_model_binary/le_16class.joblib (deflated 30%)\n",
            "  adding: content/saved_model_binary/le_4class.joblib (deflated 28%)\n",
            "  adding: content/saved_model_binary/base_model_1.joblib (deflated 82%)\n",
            "  adding: content/saved_model_4class/ (stored 0%)\n",
            "  adding: content/saved_model_4class/meta_learner.h5 (deflated 12%)\n",
            "  adding: content/saved_model_4class/selected_features.json (deflated 59%)\n",
            "  adding: content/saved_model_4class/feature_selector.joblib (deflated 44%)\n",
            "  adding: content/saved_model_4class/variance_selector.joblib (deflated 46%)\n",
            "  adding: content/saved_model_4class/base_model_2.joblib (deflated 87%)\n",
            "  adding: content/saved_model_4class/base_model_0.joblib (deflated 86%)\n",
            "  adding: content/saved_model_4class/le_binary.joblib (deflated 28%)\n",
            "  adding: content/saved_model_4class/scaler.joblib (deflated 31%)\n",
            "  adding: content/saved_model_4class/le_16class.joblib (deflated 30%)\n",
            "  adding: content/saved_model_4class/le_4class.joblib (deflated 28%)\n",
            "  adding: content/saved_model_4class/base_model_1.joblib (deflated 67%)\n",
            "  adding: content/saved_model_16class/ (stored 0%)\n",
            "  adding: content/saved_model_16class/meta_learner.h5 (deflated 11%)\n",
            "  adding: content/saved_model_16class/selected_features.json (deflated 59%)\n",
            "  adding: content/saved_model_16class/feature_selector.joblib (deflated 44%)\n",
            "  adding: content/saved_model_16class/variance_selector.joblib (deflated 46%)\n",
            "  adding: content/saved_model_16class/base_model_2.joblib (deflated 93%)\n",
            "  adding: content/saved_model_16class/base_model_0.joblib (deflated 93%)\n",
            "  adding: content/saved_model_16class/le_binary.joblib (deflated 28%)\n",
            "  adding: content/saved_model_16class/scaler.joblib (deflated 31%)\n",
            "  adding: content/saved_model_16class/le_16class.joblib (deflated 30%)\n",
            "  adding: content/saved_model_16class/le_4class.joblib (deflated 28%)\n",
            "  adding: content/saved_model_16class/base_model_1.joblib (deflated 67%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2350571b-966a-4fbf-bf36-2f693f1e3c00\", \"saved_models.zip\", 113559855)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVDCrY0XO0TeRTAXkBfcww",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}